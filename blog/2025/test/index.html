<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Test | Chiwoong Lee </title> <meta name="author" content="Chiwoong Lee"> <meta name="description" content="an example of a distill-style blog post and main elements"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chiwoonglee1.github.io/blog/2025/test/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Test",
            "description": "an example of a distill-style blog post and main elements",
            "published": "May 28, 2025",
            "authors": [
              
              {
                "author": "Chiwoong Lee",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "GSAI, POSTECH",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chiwoong</span> Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Test</h1> <p>an example of a distill-style blog post and main elements</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#equations">Equations</a> </div> <div> <a href="#citations">Citations</a> </div> <div> <a href="#footnotes">Footnotes</a> </div> <div> <a href="#code-blocks">Code Blocks</a> </div> <div> <a href="#interactive-plots">Interactive Plots</a> </div> <div> <a href="#mermaid">Mermaid</a> </div> <div> <a href="#diff2html">Diff2Html</a> </div> <div> <a href="#leaflet">Leaflet</a> </div> <div> <a href="#chartjs-echarts-and-vega-lite">Chartjs, Echarts and Vega-Lite</a> </div> <div> <a href="#tikz">TikZ</a> </div> <div> <a href="#typograms">Typograms</a> </div> <div> <a href="#layouts">Layouts</a> </div> <div> <a href="#other-typography">Other Typography?</a> </div> </nav> </d-contents> <h1 id="the-importance-of-making-models-smaller-and-faster">The Importance of Making Models Smaller and Faster</h1> <p>To use deep learning models effectively in real-world industry settings or on edge devices—such as smartphones and embedded systems—they must be not only accurate, but also fast and compact. Even lightweight CNN models like MobileNet, and EfficientNet, which are designed for efficiency, still use floating-point (FP32) operations by default. As a result, in environments with limited memory and processing power, these models can often be too large and slow to handle real-world applications.</p> <h1 id="why-convert-models-to-onnx">Why Convert Models to ONNX?</h1> <p>To deploy models developed in frameworks like PyTorch or TensorFlow, it is important to consider the variety of hardware and operating environments. ONNX (Open Neural Network Exchange) is a standard format that provides compatibility between different frameworks, offering the following advantages:</p> <ul> <li> <p><strong>Cross-framework compatibility:</strong> Converted models can be easily moved and used on different platforms.</p> </li> <li> <strong>Easy-to-use tools for better models:</strong> ONNX Runtime<a href="https://onnxruntime.ai/docs/" rel="external nofollow noopener" target="_blank">1</a> makes it simple to make your model faster and smaller.</li> <li> <strong>Hardware accelerator support:</strong> The model can run efficiently on various hardware accelerators such as NPUs and ASICs.</li> </ul> <h1 id="challenges-of-onnx-quantization">Challenges of ONNX Quantization</h1> <p>However, <strong>if you simply convert a PyTorch model to ONNX and apply the ONNX quantization, you may experience a larger than expected drop in accuracy.</strong> For example, when MobileNetV3 is quantized to INT8 using ONNX, the Top-1 accuracy can drop from 75.26% to 40.50%. This occurs because PyTorch and ONNX handle some operations differently, and the quantization process can introduce additional errors.</p> <p><strong>In this post</strong>, we explore why accuracy drops when using ONNX quantization on models like MobileNetV3 and share simple steps to reduce these issues. <strong>Our aim</strong> is to help developers and engineers deploy lightweight models in real-world apps—without big accuracy losses—by providing simple tips and effective solutions.</p> <h1 id="baseline-evaluation">Baseline Evaluation</h1> <p>In our experiments, we compared how accuracy is affected when applying ONNX quantization to various lightweight models and classic CNN architectures. All models used official PyTorch ImageNet-pretrained weights<a href="https://docs.pytorch.org/vision/0.21/models.html" rel="external nofollow noopener" target="_blank">2</a>, and we evaluated performance on the ImageNet-1K dataset (1,000 classes) using standard protocols to measure Top-1 accuracy.</p> <ul> <li> <strong>ResNet18:</strong> <code class="language-plaintext highlighter-rouge">ResNet18_Weights.IMAGENET1K_V1</code> </li> <li> <strong>MobileNetV2:</strong> <code class="language-plaintext highlighter-rouge">MobileNet_V2_Weights.IMAGENET1K_V2</code> </li> <li> <strong>MobileNetV3-Large:</strong> <code class="language-plaintext highlighter-rouge">MobileNet_V3_Large_Weights.IMAGENET1K_V2</code> </li> </ul> <p>For details on installing ONNX Runtime, converting PyTorch models to ONNX, and applying quantization, please refer to the official ONNX Runtime website<a href="https://onnxruntime.ai/docs/" rel="external nofollow noopener" target="_blank">1</a>. You can find various examples and practical guides in the official documentation.</p> <h2 id="results">Results</h2> <table> <thead> <tr> <th> </th> <th>ResNet18</th> <th>MobileNetV2</th> <th>MobileNetV3</th> </tr> </thead> <tbody> <tr> <td>Pytorch (FP32)</td> <td>69.76</td> <td>72.15</td> <td>75.26</td> </tr> <tr> <td>ONNX (INT8)</td> <td>69.36</td> <td>67.12</td> <td>42.29</td> </tr> </tbody> </table> <p><em>Table 1: Top-1 accuracy (%) of ResNet18, MobileNetV2, and MobileNetV3 on ImageNet-1K for PyTorch (FP32) and ONNX (INT8) quantized models. Note that Per-tensor quantization is the default in ONNX quantization.</em></p> <p>ResNet18: minimal drop; MobileNetV2: 5.03% drop; MobileNetV3: 32.97% drop.</p> <h1 id="what-causes-this-accuracy-drop-and-how-can-we-address-it"><u>What causes this accuracy drop, and how can we address it?</u></h1> <h2 id="1-per-channel-quantization">1. Per-Channel Quantization</h2> <p>According to the DFQ[3] paper, some models show only a small performance drop after quantization, but for models like MobileNetV2, where the weight distribution across channels is very different, simple per-tensor quantization can cause a serious drop in accuracy. For example, quantizing MobileNetV2 to 8-bit can drop Top-1 accuracy from 70.9% to 0.1% in some reported cases.</p> <p><img src="/assets/img/dfq.png" alt="Sample Image" width="700"></p> <p><em>Fig. 1: Per (output) channel weight ranges of the first depthwiseseparable layer in MobileNetV2. In the boxplot the min and max value, the 2nd and 3rd quartile and the median are plotted for each channel. This layer exhibits strong differences between channel weight ranges</em></p> <d-cite key="nagel2019data"> </d-cite> <p>[3] @inproceedings{nagel2019data, title={Data-free quantization through weight equalization and bias correction}, author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max}, booktitle={Proceedings of the IEEE/CVF international conference on computer vision}, pages={1325–1334}, year={2019} }</p> <p>Fortunately, ONNX Runtime supports per-channel quantization for operators like Conv, and you can enable this in the quantization options<a href="https://gemfury.com/turingmotors/python:onnxruntime-gpu/-/content/quantization/quantize.py" rel="external nofollow noopener" target="_blank">4</a>. Recently, many NPU (Neural Processing Unit) hardware also support per-channel quantization, allowing efficient inference with minimal accuracy loss. This technique is widely used for major operators like CNN and Fully-Connected layers.</p> <table> <thead> <tr> <th> </th> <th>ResNet18</th> <th>MobileNetV2</th> <th>MobileNetV3</th> </tr> </thead> <tbody> <tr> <td>Pytorch (FP32)</td> <td>69.76</td> <td>72.15</td> <td>75.26</td> </tr> <tr> <td>ONNX (per-tensor)</td> <td>69.36</td> <td>67.12</td> <td>42.29</td> </tr> <tr> <td>ONNX <strong>(per-channel)</strong> </td> <td>69.58</td> <td>71.61</td> <td>66.05</td> </tr> </tbody> </table> <p><em>Table 2: Top-1 accuracy (%) of ResNet18, MobileNetV2, and MobileNetV3 on ImageNet-1K for PyTorch (FP32), ONNX per-tensor quantization, and ONNX per-channel quantization.</em></p> <p>For more details on per-tensor and per-channel quantization, please refer to the paper[5]. Per-channel quantization makes the performance of ResNet and MobileNetV2 almost the same as FP, but for MobileNetV3, there is still a gap of about <strong>9.21%</strong>.</p> <p>[5] @article{nagel2021white, title={A white paper on neural network quantization}, author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and Van Baalen, Mart and Blankevoort, Tijmen}, journal={arXiv preprint arXiv:2106.08295}, year={2021} }</p> <hr> <h2 id="2-migrating-the-quantization-difficulty-from-activations-to-weights">2. Migrating the quantization difficulty from activations to weights</h2> <p>SmoothQuant[6] is a mathematical transformation that adjusts the outliers in activations by migrating the difficulty of quantization from activations to weights. While originally developed for LLMs, similar ideas have been applied to CNNs, such as the CLE (Cross-Layer Equalization) in the DFQ paper.</p> \[\mathbf{s}_j = \frac{\max(|\mathbf{X}_j|)^{\alpha}}{\max(|\mathbf{W}_j|)^{1-\alpha}} \tag{1}\] <p>CLE uses the positive scaling equivariance property of activation functions like ReLU and PReLU. By scaling the weights of one layer and applying the inverse to the next, the output of the network remains unchanged while the weight distribution becomes more balanced. This reduces quantization error caused by imbalanced weight ranges.</p> <p>We applied SmoothQuant to consecutive Conv layers with ReLU activation, migrating the difficulty of activation quantization to weights. According to the SmoothQuant paper, the hyperparameter α is sensitive. We tested α values from 0.0 to 1.0 for both per-layer and per-channel quantization settings.</p> <p>[6] @inproceedings{xiao2023smoothquant, title={Smoothquant: Accurate and efficient post-training quantization for large language models}, author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song}, booktitle={International Conference on Machine Learning}, pages={38087–38099}, year={2023}, organization={PMLR} }</p> <h2 id="results-1">Results</h2> <h4 id="per-tensor-quantization-with-smoothquant">Per-Tensor Quantization with SmoothQuant</h4> <p><img src="/assets/img/pertensor_alpha.png" alt="Sample Image" width="400"></p> <p><em>Fig. 2: <strong>Per-tensor</strong>. Top-1 accuracy of MobileNetV3 according to α, based on Eq.1.</em></p> <p>At α=0.5, accuracy improves to 68.92% (from 42.29%), making SmoothQuant a practical choice for hardware without per-channel quantization.</p> <h4 id="per-channel-quantization-with-smoothquant">Per-Channel Quantization with SmoothQuant</h4> <p><img src="/assets/img/perchannel_alpha.png" alt="Sample Image" width="400"></p> <p><em>Fig. 3: <strong>Per-channel</strong>. Top-1 accuracy of MobileNetV3 according to α, based on Eq.1.</em></p> <p>With per-channel quantization, models achieve better results and are less affected by α. The best accuracy reaches 70.27%—just 4.99% below the original FP model. When combined with SmoothQuant, accuracy improves by 27.98% (from 42.29% to 70.27%) without needing fine-tuning(QAT).</p> <h1 id="for-further-performance-improvement">For Further Performance Improvement</h1> <p>According to [7], Depthwise Separable Convolution layers tend to cause larger quantization errors compared to other layers . To address this, we excluded these layers from quantization using the nodes_to_exclude option in ONNX quantization, keeping them in FP32. Additionally, as noted in [8,9] , the first and last layers are most sensitive to information loss and quantization error, so we also kept them in FP32.</p> <p>Our experiments used per-channel quantization combined with SmoothQuant (α = 0.5) as the <em>Vanilla</em> setting.</p> <table> <thead> <tr> <th>Vanilla</th> <th>Skip Depthwise Separable Conv</th> <th>Skip First &amp; Last Layer</th> <th>Skip Both</th> </tr> </thead> <tbody> <tr> <td>70.27</td> <td>71.39</td> <td>70.74</td> <td>72.01</td> </tr> </tbody> </table> <p><em>Table 3: Top-1 accuracy (%) for different quantization strategies: Vanilla, skipping Depthwise Separable Convolution layers, skipping first and last layers, and skipping both types of layers.</em></p> <p>Excluding both layers achieved the best results, limiting the accuracy drop to just 3.25% compared to FP32. <em>Note: If your hardware strictly requires integer-only quantization (no mixed-precision operations), this approach won’t be feasible.</em></p> <p>[7] @inproceedings{yun2021all, title={Do all mobilenets quantize poorly? gaining insights into the effect of quantization on depthwise separable convolutional networks through the eyes of multi-scale distributional dynamics}, author={Yun, Stone and Wong, Alexander}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={2447–2456}, year={2021} }</p> <p>[8] @article{li2021brecq, title={Brecq: Pushing the limit of post-training quantization by block reconstruction}, author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi}, journal={arXiv preprint arXiv:2102.05426}, year={2021} } [9] @inproceedings{hubara2021accurate, title={Accurate post training quantization with small calibration sets}, author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel}, booktitle={International Conference on Machine Learning}, pages={4466–4475}, year={2021}, organization={PMLR} }</p> <h1 id="further-study">Further Study</h1> <p>Fig. 4 shows the weight distribution for some layers in MobileNetV3. Most weights are concentrated between -0.1 and 0.1, and the tensor range is very small (about 7e-8). In theory, this should be good for quantization, but in practice, ONNX Runtime quantization causes unexpected accuracy drops.</p> <p><img src="/assets/img/weight_mv3.png" alt="Sample Image" width="400"></p> <p><em>Fig. 4: Weight distribution of the features.4.block.2.fc1 layer in MobileNetV3.</em></p> <h2 id="1-bias-quantization">1. Bias Quantization</h2> <p>Typically, lower scale values indicate higher precision and less quantization error. However, in ONNX Runtime, problems can occur when the bias is quantized to int32 for integer-only quantization.</p> <p>This issue arises from how the bias scale is calculated. In some MobileNetV3 layers, the weight scale can be extremely small (around 1e-8), which makes the bias scale as tiny as 1e-15. As a result, the quantized bias value may exceed the INT32 range (-2,147,483,648 to 2,147,483,647), causing saturation and loss of information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate scale for bias
</span><span class="n">bias_scale</span> <span class="o">=</span> <span class="n">input_scale</span> <span class="o">*</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">beta</span>
<span class="n">quantized_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">bias_data</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_scale</span><span class="p">).</span><span class="nf">round</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div></div> <h2 id="2-simple-solution-weight-pruning">2. Simple Solution: Weight Pruning</h2> <h3 id="handling-small-weights">Handling Small Weights</h3> <p>For layers with very small weights, we can set weights below a certain value (e.g., 1e-7) to zero. This makes the weight scale equal to 1, so the bias scale depends only on the input scale, which helps reduce saturation problems.</p> <p><img src="/assets/img/bias_scale.png" alt="Sample Image" width="400"></p> <p><em>Fig. 5: The quantized bias values in int32 format <strong>(left)</strong> and the corresponding weight_scale values <strong>(right)</strong>.</em></p> <h3 id="results-2">Results</h3> <p>After applying this method, accuracy improved from 66.05% to 69.2%, a gain of 3.15%. However, setting weights to zero can slightly reduce the original model’s accuracy (from 75.266% to 75.26%). So, it is important to choose which weights to prune carefully. Since this post focuses on quantization, we will not go into the details of pruning.</p> <h1 id="future-work--discussion">Future Work &amp; Discussion</h1> <p>SmoothQuant and similar methods are effective for CNN models like MobileNet, especially when per-channel quantization is not supported by hardware. However, SmoothQuant can only be applied to layers with ReLU activation, and not to layers with <em>Hswish</em> or <em>Hardsigmoid</em>, which are used in MobileNet. For these layers, further research is needed to improve performance. Moreover, integer-only quantization in practice involves many factors to consider, such as bias quantization. If you encounter similar accuracy drops in real-world applications, we hope the guidance provided in this blog will serve as a useful reference to help address these challenges.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'chiwoonglee1/chiwoonglee1.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Chiwoong Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
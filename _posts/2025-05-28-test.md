---
layout: distill
title: How to Enhance Model Performance in ONNX Quantization
description: .
tags: distill formatting
giscus_comments: true
date: 2025-05-28
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Chiwoong Lee
    url: ""
    affiliations:
      name: GSAI, POSTECH

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: The Importance of Making Models Smaller and Faster
  - name: Why Convert Models to ONNX?
  - name: Challenges of ONNX Quantization
  - name: Baseline Evaluation
    subsections:
      - name: Results
  - name: What causes this accuracy drop, and how can we address it?
  - name: For Further Performance Improvement
  - name: Further Study
  - name: Future Work & Discussion

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
## The Importance of Making Models Smaller and Faster
To use deep learning models effectively in real-world industry settings or on edge devices—such as smartphones and embedded systems—they must be not only accurate, but also fast and compact. Even lightweight CNN models like MobileNet, and EfficientNet, which are designed for efficiency, still use floating-point (FP32) operations by default. As a result, in environments with limited memory and processing power, these models can often be too large and slow to handle real-world applications.

## Why Convert Models to ONNX?
To deploy models developed in frameworks like PyTorch or TensorFlow, it is important to consider the variety of hardware and operating environments. ONNX (Open Neural Network Exchange) is a standard format that provides compatibility between different frameworks, offering the following advantages:

- **Cross-framework compatibility:** Converted models can be easily moved and used on different platforms.

- **Easy-to-use tools for better models:** ONNX Runtime[1] makes it simple to make your model faster and smaller.
- **Hardware accelerator support:** The model can run efficiently on various hardware accelerators such as NPUs and ASICs.

[1]: https://onnxruntime.ai/docs/

## Challenges of ONNX Quantization
However, **if you simply convert a PyTorch model to ONNX and apply the ONNX quantization, you may experience a larger than expected drop in accuracy.** For example, when MobileNetV3 is quantized to INT8 using ONNX, the Top-1 accuracy can drop from 75.26% to 40.50%. This occurs because PyTorch and ONNX handle some operations differently, and the quantization process can introduce additional errors.

**In this post**, we explore why accuracy drops when using ONNX quantization on models like MobileNetV3 and share simple steps to reduce these issues. **Our aim** is to help developers and engineers deploy lightweight models in real-world apps—without big accuracy losses—by providing simple tips and effective solutions.

## Baseline Evaluation
In our experiments, we compared how accuracy is affected when applying ONNX quantization to various lightweight models and classic CNN architectures. All models used official PyTorch ImageNet-pretrained weights[2], and we evaluated performance on the ImageNet-1K dataset (1,000 classes) using standard protocols to measure Top-1 accuracy.

- **ResNet18:** `ResNet18_Weights.IMAGENET1K_V1`
- **MobileNetV2:** `MobileNet_V2_Weights.IMAGENET1K_V2`
- **MobileNetV3-Large:** `MobileNet_V3_Large_Weights.IMAGENET1K_V2`

For details on installing ONNX Runtime, converting PyTorch models to ONNX, and applying quantization, please refer to the official ONNX Runtime website[1].
You can find various examples and practical guides in the official documentation.


[2]: https://docs.pytorch.org/vision/0.21/models.html

### Results

|                  | ResNet18 | MobileNetV2 | MobileNetV3 |
|------------------|-------------|----------------|----------------|
| Pytorch (FP32)       | 69.76|72.15| 75.26 |
| ONNX (INT8)          | 69.36|67.12| 42.29|

*Table 1: Top-1 accuracy (%) of ResNet18, MobileNetV2, and MobileNetV3 on ImageNet-1K for PyTorch (FP32) and ONNX (INT8) quantized models. Note that Per-tensor quantization is the default in ONNX quantization.*

ResNet18: minimal drop; MobileNetV2: 5.03% drop; MobileNetV3: 32.97% drop.

# <u>What causes this accuracy drop, and how can we address it?</u>
[What causes this accuracy drop, and how can we address it?](https://chiwoonglee1.github.io/blog/2025/test/#what-causes-this-accuracy-drop-and-how-can-we-address-it/)

### 1. Per-Channel Quantization
According to the DFQ <d-cite key=nagel2019data></d-cite> paper, some models show only a small performance drop after quantization, but for models like MobileNetV2, where the weight distribution across channels is very different, simple per-tensor quantization can cause a serious drop in accuracy. For example, quantizing MobileNetV2 to 8-bit can drop Top-1 accuracy from 70.9% to 0.1% in some reported cases.

<img src="/assets/img/dfq.png" alt="Sample Image" width="700"/>

*Fig. 1: Per (output) channel weight ranges of the first depthwiseseparable layer in MobileNetV2. In the boxplot the min and max value, the 2nd and 3rd quartile and the median are plotted for each
channel. This layer exhibits strong differences between channel weight ranges*



<!-- 
[3] 
@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1325--1334},
  year={2019}
} -->



Fortunately, ONNX Runtime supports per-channel quantization for operators like Conv, and you can enable this in the quantization options[4]. Recently, many NPU (Neural Processing Unit) hardware also support per-channel quantization, allowing efficient inference with minimal accuracy loss. This technique is widely used for major operators like CNN and Fully-Connected layers.


[4]: https://gemfury.com/turingmotors/python:onnxruntime-gpu/-/content/quantization/quantize.py


|                  | ResNet18 | MobileNetV2 | MobileNetV3 |
|------------------|-------------|----------------|----------------|
| Pytorch (FP32)       | 69.76|72.15| 75.26 |
| ONNX (per-tensor)          | 69.36|67.12| 42.29|
| ONNX **(per-channel)** | 69.58|71.61|66.05|

*Table 2: Top-1 accuracy (%) of ResNet18, MobileNetV2, and MobileNetV3 on ImageNet-1K for PyTorch (FP32), ONNX per-tensor quantization, and ONNX per-channel quantization.*


For more details on per-tensor and per-channel quantization, please refer to the paper <d-cite key=nagel2021white></d-cite>.
Per-channel quantization makes the performance of ResNet and MobileNetV2 almost the same as FP, but for MobileNetV3, there is still a gap of about **9.21%**.

<!-- [5]
@article{nagel2021white,
  title={A white paper on neural network quantization},
  author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and Van Baalen, Mart and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2106.08295},
  year={2021}
} -->

---

### 2. Migrating the quantization difficulty from activations to weights
SmoothQuant<d-cite key=xiao2023smoothquant></d-cite> is a mathematical transformation that adjusts the outliers in activations by migrating the difficulty of quantization from activations to weights. While originally developed for LLMs, similar ideas have been applied to CNNs, such as the CLE (Cross-Layer Equalization) in the DFQ paper.


$$
\mathbf{s}_j = \frac{\max(|\mathbf{X}_j|)^{\alpha}}{\max(|\mathbf{W}_j|)^{1-\alpha}} \tag{1}
$$

CLE uses the positive scaling equivariance property of activation functions like ReLU and PReLU. By scaling the weights of one layer and applying the inverse to the next, the output of the network remains unchanged while the weight distribution becomes more balanced. This reduces quantization error caused by imbalanced weight ranges.

We applied SmoothQuant to consecutive Conv layers with ReLU activation, migrating the difficulty of activation quantization to weights. According to the SmoothQuant paper, the hyperparameter α is sensitive. We tested α values from 0.0 to 1.0 for both per-layer and per-channel quantization settings.




<!-- [6]
@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
} -->

### Results

#### Per-Tensor Quantization with SmoothQuant

<img src="/assets/img/pertensor_alpha.png" alt="Sample Image" width="400"/>

*Fig. 2: **Per-tensor**. Top-1 accuracy of MobileNetV3 according to α, based on Eq.1.*





At α=0.5, accuracy improves to 68.92% (from 42.29%), making SmoothQuant a practical choice for hardware without per-channel quantization.

#### Per-Channel Quantization with SmoothQuant
<img src="/assets/img/perchannel_alpha.png" alt="Sample Image" width="400"/>



*Fig. 3: **Per-channel**. Top-1 accuracy of MobileNetV3 according to α, based on Eq.1.*



With per-channel quantization, models achieve better results and are less affected by α. The best accuracy reaches 70.27%—just 4.99% below the original FP model. When combined with SmoothQuant, accuracy improves by 27.98% (from 42.29% to 70.27%) without needing fine-tuning(QAT).

## For Further Performance Improvement
According to <d-cite key=yun2021all></d-cite>, Depthwise Separable Convolution layers tend to cause larger quantization errors compared to other layers . To address this, we excluded these layers from quantization using the nodes_to_exclude option in ONNX quantization, keeping them in FP32. Additionally, as noted in <d-cite key=li2021brecq></d-cite> <d-cite key=hubara2021accurate></d-cite> , the first and last layers are most sensitive to information loss and quantization error, so we also kept them in FP32.

Our experiments used per-channel quantization combined with SmoothQuant (α = 0.5) as the *Vanilla* setting.

| Vanilla                        | Skip Depthwise Separable Conv | Skip First & Last Layer | Skip Both                |
|---------------------------------|------------------------------|-------------------------|--------------------------|
| 70.27                          | 71.39                        | 70.74                   | 72.01                    |

*Table 3: Top-1 accuracy (%) for different quantization strategies: Vanilla, skipping Depthwise Separable Convolution layers, skipping first and last layers, and skipping both types of layers.*


Excluding both layers achieved the best results, limiting the accuracy drop to just 3.25% compared to FP32.
*Note: If your hardware strictly requires integer-only quantization (no mixed-precision operations), this approach won’t be feasible.*


<!-- [7]
@inproceedings{yun2021all,
  title={Do all mobilenets quantize poorly? gaining insights into the effect of quantization on depthwise separable convolutional networks through the eyes of multi-scale distributional dynamics},
  author={Yun, Stone and Wong, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2447--2456},
  year={2021}
} -->
<!-- 
[8]
@article{li2021brecq,
  title={Brecq: Pushing the limit of post-training quantization by block reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  journal={arXiv preprint arXiv:2102.05426},
  year={2021}
} -->
<!-- [9]
@inproceedings{hubara2021accurate,
  title={Accurate post training quantization with small calibration sets},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4466--4475},
  year={2021},
  organization={PMLR}
} -->

## Further Study

[Further Study](https://chiwoonglee1.github.io/blog/2025/test/#further-study/)
Fig. 4 shows the weight distribution for some layers in MobileNetV3. Most weights are concentrated between -0.1 and 0.1, and the tensor range is very small (about 7e-8). In theory, this should be good for quantization, but in practice, ONNX Runtime quantization causes unexpected accuracy drops.


<img src="/assets/img/weight_mv3.png" alt="Sample Image" width="400"/>

*Fig. 4: Weight distribution of the features.4.block.2.fc1 layer in MobileNetV3.*


## 1. Bias Quantization

Typically, lower scale values indicate higher precision and less quantization error. However, in ONNX Runtime, problems can occur when the bias is quantized to int32 for integer-only quantization.

This issue arises from how the bias scale is calculated. In some MobileNetV3 layers, the weight scale can be extremely small (around 1e-8), which makes the bias scale as tiny as 1e-15. As a result, the quantized bias value may exceed the INT32 range (-2,147,483,648 to 2,147,483,647), causing saturation and loss of information.

```python
# calculate scale for bias
bias_scale = input_scale * weight_scale * beta
quantized_data = (np.asarray(bias_data) / bias_scale).round().astype(np.int32)
```

### 2. Simple Solution: Weight Pruning

#### Handling Small Weights
For layers with very small weights, we can set weights below a certain value (e.g., 1e-7) to zero. This makes the weight scale equal to 1, so the bias scale depends only on the input scale, which helps reduce saturation problems.

<img src="/assets/img/bias_scale.png" alt="Sample Image" width="400"/>

*Fig. 5: The quantized bias values in int32 format **(left)** and the corresponding weight_scale values **(right)**.*


#### Results
After applying this method, accuracy improved from 66.05% to 69.2%, a gain of 3.15%. However, setting weights to zero can slightly reduce the original model’s accuracy (from 75.266% to 75.26%). So, it is important to choose which weights to prune carefully. Since this post focuses on quantization, we will not go into the details of pruning.

## Future Work & Discussion
[Future Work & Discussion](https://chiwoonglee1.github.io/blog/2025/test/#future-work-&discussion) Future Work & Discussion SmoothQuant and similar methods are effective for CNN models like MobileNet, especially when per-channel quantization is not supported by hardware. However, SmoothQuant can only be applied to layers with ReLU activation, and not to layers with *Hswish* or *Hardsigmoid*, which are used in MobileNet. For these layers, further research is needed to improve performance.
Moreover, integer-only quantization in practice involves many factors to consider, such as bias quantization. If you encounter similar accuracy drops in real-world applications, we hope the guidance provided in this blog will serve as a useful reference to help address these challenges.

<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://chiwoonglee1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chiwoonglee1.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-29T17:10:14+00:00</updated><id>https://chiwoonglee1.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How to Minimize Accuracy Drop in ONNX Model Quantization</title><link href="https://chiwoonglee1.github.io/blog/2025/test/" rel="alternate" type="text/html" title="How to Minimize Accuracy Drop in ONNX Model Quantization"/><published>2025-05-28T00:00:00+00:00</published><updated>2025-05-28T00:00:00+00:00</updated><id>https://chiwoonglee1.github.io/blog/2025/test</id><content type="html" xml:base="https://chiwoonglee1.github.io/blog/2025/test/"><![CDATA[<h1 id="the-importance-of-making-models-smaller-and-faster">The Importance of Making Models Smaller and Faster</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#the-importance-of-making-models-smaller-and-faster/">The Importance of Making Models Smaller and Faster</a> To use deep learning models effectively in real-world industry settings or on edge devices—such as smartphones and embedded systems—they must be not only accurate, but also fast and compact. Even lightweight CNN models like MobileNet, and EfficientNet, which are designed for efficiency, still use floating-point (FP32) operations by default. As a result, in environments with limited memory and processing power, these models can often be too large and slow to handle real-world applications.</p> <h1 id="why-convert-models-to-onnx">Why Convert Models to ONNX?</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#why-convert-models-to-onnx/">Why Convert Models to ONNX?</a> To deploy models developed in frameworks like PyTorch or TensorFlow, it is important to consider the variety of hardware and operating environments. ONNX (Open Neural Network Exchange) is a standard format that provides compatibility between different frameworks, offering the following advantages:</p> <ul> <li> <p><strong>Cross-framework compatibility:</strong> Converted models can be easily moved and used on different platforms.</p> </li> <li><strong>Easy-to-use tools for better models:</strong> ONNX Runtime<a href="https://onnxruntime.ai/docs/">1</a> makes it simple to make your model faster and smaller.</li> <li><strong>Hardware accelerator support:</strong> The model can run efficiently on various hardware accelerators such as NPUs and ASICs.</li> </ul> <h1 id="challenges-of-onnx-quantization">Challenges of ONNX Quantization</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#challenges-of-onnx-quantization/">Challenges of ONNX Quantization</a> However, <strong>if you simply convert a PyTorch model to ONNX and apply the ONNX quantization, you may experience a larger than expected drop in accuracy.</strong> For example, when MobileNetV3 is quantized to INT8 using ONNX, the Top-1 accuracy can drop from 75.26% to 40.50%. This occurs because PyTorch and ONNX handle some operations differently, and the quantization process can introduce additional errors.</p> <p><strong>In this post</strong>, we explore why accuracy drops when using ONNX quantization on models like MobileNetV3 and share simple steps to reduce these issues. <strong>Our aim</strong> is to help developers and engineers deploy lightweight models in real-world apps—without big accuracy losses—by providing simple tips and effective solutions.</p> <h1 id="baseline-evaluation">Baseline Evaluation</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#baseline-evaluation/">Baseline Evaluation</a> In our experiments, we compared how accuracy is affected when applying ONNX quantization to various lightweight models and classic CNN architectures. All models used official PyTorch ImageNet-pretrained weights<a href="https://docs.pytorch.org/vision/0.21/models.html">2</a>, and we evaluated performance on the ImageNet-1K dataset (1,000 classes) using standard protocols to measure Top-1 accuracy.</p> <ul> <li><strong>ResNet18:</strong> <code class="language-plaintext highlighter-rouge">ResNet18_Weights.IMAGENET1K_V1</code></li> <li><strong>MobileNetV2:</strong> <code class="language-plaintext highlighter-rouge">MobileNet_V2_Weights.IMAGENET1K_V2</code></li> <li><strong>MobileNetV3-Large:</strong> <code class="language-plaintext highlighter-rouge">MobileNet_V3_Large_Weights.IMAGENET1K_V2</code></li> </ul> <p>For details on installing ONNX Runtime, converting PyTorch models to ONNX, and applying quantization, please refer to the official ONNX Runtime website<a href="https://onnxruntime.ai/docs/">1</a>. You can find various examples and practical guides in the official documentation.</p> <h2 id="results">Results</h2> <table> <thead> <tr> <th> </th> <th>ResNet18</th> <th>MobileNetV2</th> <th>MobileNetV3</th> </tr> </thead> <tbody> <tr> <td>Pytorch (FP32)</td> <td>69.76</td> <td>72.15</td> <td>75.26</td> </tr> <tr> <td>ONNX (INT8)</td> <td>69.36</td> <td>67.12</td> <td>42.29</td> </tr> </tbody> </table> <p><em>Table 1: Top-1 accuracy (%) of ResNet18, MobileNetV2, and MobileNetV3 on ImageNet-1K for PyTorch (FP32) and ONNX (INT8) quantized models. Note that Per-tensor quantization is the default in ONNX quantization.</em></p> <p>ResNet18: minimal drop; MobileNetV2: 5.03% drop; MobileNetV3: 32.97% drop.</p> <h1 id="what-causes-this-accuracy-drop-and-how-can-we-address-it"><u>What causes this accuracy drop, and how can we address it?</u></h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#what-causes-this-accuracy-drop-and-how-can-we-address-it/">What causes this accuracy drop, and how can we address it?</a></p> <h2 id="1-per-channel-quantization">1. Per-Channel Quantization</h2> <p>According to the DFQ <d-cite key="nagel2019data"></d-cite> paper, some models show only a small performance drop after quantization, but for models like MobileNetV2, where the weight distribution across channels is very different, simple per-tensor quantization can cause a serious drop in accuracy. For example, quantizing MobileNetV2 to 8-bit can drop Top-1 accuracy from 70.9% to 0.1% in some reported cases.</p> <p><img src="/assets/img/dfq.png" alt="Sample Image" width="700"/></p> <p><em>Fig. 1: Per (output) channel weight ranges of the first depthwiseseparable layer in MobileNetV2. In the boxplot the min and max value, the 2nd and 3rd quartile and the median are plotted for each channel. This layer exhibits strong differences between channel weight ranges</em></p> <p>Fortunately, ONNX Runtime supports per-channel quantization for operators like Conv, and you can enable this in the quantization options<a href="https://gemfury.com/turingmotors/python:onnxruntime-gpu/-/content/quantization/quantize.py">4</a>. Recently, many NPU (Neural Processing Unit) hardware also support per-channel quantization, allowing efficient inference with minimal accuracy loss. This technique is widely used for major operators like CNN and Fully-Connected layers.</p> <table> <thead> <tr> <th> </th> <th>ResNet18</th> <th>MobileNetV2</th> <th>MobileNetV3</th> </tr> </thead> <tbody> <tr> <td>Pytorch (FP32)</td> <td>69.76</td> <td>72.15</td> <td>75.26</td> </tr> <tr> <td>ONNX (per-tensor)</td> <td>69.36</td> <td>67.12</td> <td>42.29</td> </tr> <tr> <td>ONNX <strong>(per-channel)</strong></td> <td>69.58</td> <td>71.61</td> <td>66.05</td> </tr> </tbody> </table> <p><em>Table 2: Top-1 accuracy (%) of ResNet18, MobileNetV2, and MobileNetV3 on ImageNet-1K for PyTorch (FP32), ONNX per-tensor quantization, and ONNX per-channel quantization.</em></p> <p>For more details on per-tensor and per-channel quantization, please refer to the paper <d-cite key="nagel2021white"></d-cite>. Per-channel quantization makes the performance of ResNet and MobileNetV2 almost the same as FP, but for MobileNetV3, there is still a gap of about <strong>9.21%</strong>.</p> <hr/> <h2 id="2-migrating-the-quantization-difficulty-from-activations-to-weights">2. Migrating the quantization difficulty from activations to weights</h2> <p>SmoothQuant<d-cite key="xiao2023smoothquant"></d-cite> is a mathematical transformation that adjusts the outliers in activations by migrating the difficulty of quantization from activations to weights. While originally developed for LLMs, similar ideas have been applied to CNNs, such as the CLE (Cross-Layer Equalization) in the DFQ paper.</p> \[\mathbf{s}_j = \frac{\max(|\mathbf{X}_j|)^{\alpha}}{\max(|\mathbf{W}_j|)^{1-\alpha}} \tag{1}\] <p>CLE uses the positive scaling equivariance property of activation functions like ReLU and PReLU. By scaling the weights of one layer and applying the inverse to the next, the output of the network remains unchanged while the weight distribution becomes more balanced. This reduces quantization error caused by imbalanced weight ranges.</p> <p>We applied SmoothQuant to consecutive Conv layers with ReLU activation, migrating the difficulty of activation quantization to weights. According to the SmoothQuant paper, the hyperparameter α is sensitive. We tested α values from 0.0 to 1.0 for both per-layer and per-channel quantization settings.</p> <h2 id="results-1">Results</h2> <h4 id="per-tensor-quantization-with-smoothquant">Per-Tensor Quantization with SmoothQuant</h4> <p><img src="/assets/img/pertensor_alpha.png" alt="Sample Image" width="400"/></p> <p><em>Fig. 2: <strong>Per-tensor</strong>. Top-1 accuracy of MobileNetV3 according to α, based on Eq.1.</em></p> <p>At α=0.5, accuracy improves to 68.92% (from 42.29%), making SmoothQuant a practical choice for hardware without per-channel quantization.</p> <h4 id="per-channel-quantization-with-smoothquant">Per-Channel Quantization with SmoothQuant</h4> <p><img src="/assets/img/perchannel_alpha.png" alt="Sample Image" width="400"/></p> <p><em>Fig. 3: <strong>Per-channel</strong>. Top-1 accuracy of MobileNetV3 according to α, based on Eq.1.</em></p> <p>With per-channel quantization, models achieve better results and are less affected by α. The best accuracy reaches 70.27%—just 4.99% below the original FP model. When combined with SmoothQuant, accuracy improves by 27.98% (from 42.29% to 70.27%) without needing fine-tuning(QAT).</p> <h1 id="for-further-performance-improvement">For Further Performance Improvement</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#for-further-performance-improvement/">For Further Performance Improvement</a> According to <d-cite key="yun2021all"></d-cite>, Depthwise Separable Convolution layers tend to cause larger quantization errors compared to other layers . To address this, we excluded these layers from quantization using the nodes_to_exclude option in ONNX quantization, keeping them in FP32. Additionally, as noted in <d-cite key="li2021brecq"></d-cite> <d-cite key="hubara2021accurate"></d-cite> , the first and last layers are most sensitive to information loss and quantization error, so we also kept them in FP32.</p> <p>Our experiments used per-channel quantization combined with SmoothQuant (α = 0.5) as the <em>Vanilla</em> setting.</p> <table> <thead> <tr> <th>Vanilla</th> <th>Skip Depthwise Separable Conv</th> <th>Skip First &amp; Last Layer</th> <th>Skip Both</th> </tr> </thead> <tbody> <tr> <td>70.27</td> <td>71.39</td> <td>70.74</td> <td>72.01</td> </tr> </tbody> </table> <p><em>Table 3: Top-1 accuracy (%) for different quantization strategies: Vanilla, skipping Depthwise Separable Convolution layers, skipping first and last layers, and skipping both types of layers.</em></p> <p>Excluding both layers achieved the best results, limiting the accuracy drop to just 3.25% compared to FP32. <em>Note: If your hardware strictly requires integer-only quantization (no mixed-precision operations), this approach won’t be feasible.</em></p> <h1 id="further-study">Further Study</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#further-study/">Further Study</a> Fig. 4 shows the weight distribution for some layers in MobileNetV3. Most weights are concentrated between -0.1 and 0.1, and the tensor range is very small (about 7e-8). In theory, this should be good for quantization, but in practice, ONNX Runtime quantization causes unexpected accuracy drops.</p> <p><img src="/assets/img/weight_mv3.png" alt="Sample Image" width="400"/></p> <p><em>Fig. 4: Weight distribution of the features.4.block.2.fc1 layer in MobileNetV3.</em></p> <h2 id="1-bias-quantization">1. Bias Quantization</h2> <p>Typically, lower scale values indicate higher precision and less quantization error. However, in ONNX Runtime, problems can occur when the bias is quantized to int32 for integer-only quantization.</p> <p>This issue arises from how the bias scale is calculated. In some MobileNetV3 layers, the weight scale can be extremely small (around 1e-8), which makes the bias scale as tiny as 1e-15. As a result, the quantized bias value may exceed the INT32 range (-2,147,483,648 to 2,147,483,647), causing saturation and loss of information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate scale for bias
</span><span class="n">bias_scale</span> <span class="o">=</span> <span class="n">input_scale</span> <span class="o">*</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">beta</span>
<span class="n">quantized_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">bias_data</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_scale</span><span class="p">).</span><span class="nf">round</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div></div> <h2 id="2-simple-solution-weight-pruning">2. Simple Solution: Weight Pruning</h2> <h3 id="handling-small-weights">Handling Small Weights</h3> <p>For layers with very small weights, we can set weights below a certain value (e.g., 1e-7) to zero. This makes the weight scale equal to 1, so the bias scale depends only on the input scale, which helps reduce saturation problems.</p> <p><img src="/assets/img/bias_scale.png" alt="Sample Image" width="400"/></p> <p><em>Fig. 5: The quantized bias values in int32 format <strong>(left)</strong> and the corresponding weight_scale values <strong>(right)</strong>.</em></p> <h3 id="results-2">Results</h3> <p>After applying this method, accuracy improved from 66.05% to 69.2%, a gain of 3.15%. However, setting weights to zero can slightly reduce the original model’s accuracy (from 75.266% to 75.26%). So, it is important to choose which weights to prune carefully. Since this post focuses on quantization, we will not go into the details of pruning.</p> <h1 id="future-work--discussionㄴ">Future Work &amp; Discussionㄴ</h1> <p><a href="https://chiwoonglee1.github.io/blog/2025/test/#future-work-&amp;discussion">Future Work &amp; Discussion</a> Future Work &amp; Discussion SmoothQuant and similar methods are effective for CNN models like MobileNet, especially when per-channel quantization is not supported by hardware. However, SmoothQuant can only be applied to layers with ReLU activation, and not to layers with <em>Hswish</em> or <em>Hardsigmoid</em>, which are used in MobileNet. For these layers, further research is needed to improve performance. Moreover, integer-only quantization in practice involves many factors to consider, such as bias quantization. If you encounter similar accuracy drops in real-world applications, we hope the guidance provided in this blog will serve as a useful reference to help address these challenges.</p>]]></content><author><name>Chiwoong Lee</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://chiwoonglee1.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://chiwoonglee1.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://chiwoonglee1.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://chiwoonglee1.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://chiwoonglee1.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://chiwoonglee1.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>